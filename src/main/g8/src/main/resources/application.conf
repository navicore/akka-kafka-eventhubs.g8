main {

  appName = "akka-kafka-eventhubs"
  appName = \${?APP_NAME}

  logLevel = "debug"
  logLevel = \${?LOG_LEVEL}

}

eventhubs {

  // for publishing to eventhubs
  connStr = "CHANGE_ME"
  connStr = \${?EVENTHUBS_CONNSTR}

}

akka {

  loglevel = "DEBUG"
  loglevel = \${?AKKA_LOG_LEVEL}
  stdout-loglevel = "DEBUG"
  stdout-loglevel = \${?STDOUT_LOG_LEVEL}

  actor {
    provider = akka.actor.LocalActorRefProvider
    #provider = cluster
    #provider = remote
  }

  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      #port = 2552
      port = 0
    }
  }

  cluster {
    seed-nodes = [
      "akka.tcp://ClusterSystem@127.0.0.1:2551",
      "akka.tcp://ClusterSystem@127.0.0.1:2552"
    ]
  }

}

// @see http://doc.akka.io/docs/akka/2.4.10/scala/logging.html
akka {
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = \${?AKKA_LOG_LEVEL}
  loglevel = "INFO"
}

eventhub-react {

  // Connection settings can be retrieved from the Azure portal at https://portal.azure.com
  connection {

    // your-namespace.servicebus.windows.net
    eventHubEndpoint = \${?EVENTHUB_ENDPOINT}

    // eh1
    eventHubName = \${?EVENTHUB_NAME}

    // 0..n
    eventHubPartitions = \${?EVENTHUB_PARTITIONS}

    // SAS policy name
    accessPolicy = \${?EVENTHUB_ACCESS_POLICY}

    // policy key
    accessKey = \${?EVENTHUB_ACCESS_KEY}
  }

  streaming {

    // "\$Default" is predefined and is the typical scenario
    consumerGroup = "\$Default"

    // Value expressed as a duration, e.g. 3s, 3000ms, "3 seconds", etc.
    receiverTimeout = 3s

    // How many messages to retrieve on each pull, max is 999
    receiverBatchSize = 999

    // Whether to retrieve information about the partitions while streaming events from Event Hub
    retrieveRuntimeInfo = true
  }

  checkpointing {

    // Checkpoints frequency (best effort), for each Event hub partition
    // Min: 1 second, Max: 1 minute
    frequency = 5s

    // How many messages to stream before saving the position, for each Event hub partition.
    // Since the stream position is saved in the Source, before the rest of the
    // Graph (Flows/Sinks), this provides a mechanism to replay buffered messages.
    // The value should be greater than receiverBatchSize
    countThreshold = 5

    // Store a position if its value is older than this amount of time, ignoring the threshold.
    // For instance when the telemetry stops, this will force to write the last offset after some time.
    // Min: 1 second, Max: 1 hour. Value is rounded to seconds.
    timeThreshold = 10s

    storage {

      // Value expressed as a duration, e.g. 3s, 3000ms, "3 seconds", etc.
      rwTimeout = 5s

      // Supported types (not case sensitive): Cassandra, AzureBlob
      // backendType = "Cassandra"
      backendType = "AzureBlob"

      // If you use the same storage while processing multiple streams, you'll want
      // to use a distinct table/container/path in each job, to to keep state isolated
      namespace = "eventhub-react-checkpoints"

      azureblob {
        // Time allowed for a checkpoint to be written, rounded to seconds (min 15, max 60)
        lease = 15s
        // Whether to use the Azure Storage Emulator
        useEmulator = false
        // Storage credentials
        protocol = "https"
        account = \${?CHECKPOINT_ACCOUNT}
        key = \${?CHECKPOINT_KEY}
      }

      // You can easily test this with Docker --> docker run -ip 9042:9042 --rm cassandra
      cassandra {
        cluster = "localhost:9042"
        replicationFactor = 1
        username = ""
        password = ""
      }
    }
  }
}

kafka {

  parallelism = "4"
  parallelism = \${?PARALLELISM }

  bootstrap = "localhost:9092"
  bootstrap = \${?BOOTSTRAP}

  consumerGroup = "\$Default"
  consumerGroup = \${?CONSUMER_GROUP}

  topic = "\$Default"
  topic = \${?TOPIC}

}

# Properties for akka.kafka.ConsumerSettings can be
# defined in this section or a configuration section with
# the same layout.
akka.kafka.consumer {
  # Tuning property of scheduled polls.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that blocking of the thread that
  # is executing the stage will be blocked.
  poll-timeout = 50ms

  # The stage will be await outstanding offset commit requests before
  # shutting down, but if that takes longer than this timeout it will
  # stop forcefully.
  stop-timeout = 30s

  # How long to wait for `KafkaConsumer.close`
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `TimeoutException`.
  commit-timeout = 15s

  # If the KafkaConsumer can't connect to the broker the poll will be
  # aborted after this timeout. The KafkaConsumerActor will throw
  # org.apache.kafka.common.errors.WakeupException which will be ignored
  # until max-wakeups limit gets exceeded.
  wakeup-timeout = 3s

  # After exceeding maxinum wakeups the kafka will stop and the stage will fail.
  max-wakeups = 10

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.kafka.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
  }
}

